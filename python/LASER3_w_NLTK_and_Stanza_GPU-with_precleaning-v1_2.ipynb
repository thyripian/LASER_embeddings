{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f5a66e",
   "metadata": {},
   "source": [
    "# <center> LASER EMBEDDINGS WITH API v1.2</center>\n",
    "---\n",
    "\n",
    "###### <center>+ LangDetect and LangId for sentence-level language detection and NLTK and Stanza for language-specific sentence tokenization.</center>\n",
    "---\n",
    "<center><b>Created by:<b></center></br>\n",
    "<center>Kevan White, Sr. Data Scientist (thyripian)</center></br>\n",
    "<center>Release Date: 22 JUN 2023</center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffbc5e5",
   "metadata": {},
   "source": [
    "---\n",
    "#### <center>Notes:</center>\n",
    "This python 'module' relies on the LASER_embeddings repository and associated Docker Image to run. Failure to clone the repository and build the Docker Image will result in failed attempts to process the data.</br>\n",
    "If you have cloned the repository, but are unaware of how to build the Docker Image:</br>\n",
    "- Install Docker Desktop (will require a system restart or log-out)\n",
    "- Using a Bash terminal (such as GitBASH), navigate to the directory of the cloned repository containing the .dockerfile\n",
    "- Run the command:   docker build -t LASER_embeddings -f LASER_embeddings.dockerfile .\n",
    "- Once the Image is built, you may need to restart the computer and Docker Desktop.\n",
    "- Reopen the Bash terminal and navigate back to the same directory.\n",
    "- Run the command:   docker run -it -p 8080:80 --gpus 0 LASER_embeddings\n",
    "- Once this last command is run, you can start and stop the Docker Image in the Docker Desktop GUI for all future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1f7f3",
   "metadata": {},
   "source": [
    "---\n",
    "### <center> Imports and Setup</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import nltk\n",
    "import requests\n",
    "import json\n",
    "import numpy as np \n",
    "from IPython.display import Audio\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "from langid import classify\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ab2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Get the current GPU device\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "# Get the name of the current GPU\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# Get the GPU memory usage\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "# API call to use LASER3 with Ed's API.\n",
    "def LASER_api_call(payload):\n",
    "    url = 'http://localhost:8080/vectorize'\n",
    "    \n",
    "    headers = {\n",
    "    'Content-Type':'application/json',\n",
    "    'Accept':'*/*',\n",
    "    'Accept-Encoding':'gzip,deflate,br',\n",
    "    'Connection':'keep-alive'\n",
    "    }\n",
    "    \n",
    "    # Convert payload to JSON\n",
    "    json_payload = json.dumps(payload)\n",
    "    response = requests.get(url,headers=headers,data=json_payload)\n",
    "    \n",
    "    # If good HTTP return, pull the content.\n",
    "    if response.status_code == 200:\n",
    "        content = response.content\n",
    "        data = json.loads(content)\n",
    "        return data\n",
    "    \n",
    "    # Other log error.\n",
    "    else:\n",
    "        try:\n",
    "            error_msg = f'Error: {response.status_code}'\n",
    "        except:\n",
    "            # Added because there was a repeat issue for one of the TWN \n",
    "            # entries with the previous error_msg method.\n",
    "            error_msg = 'Error with item. Skipping.' \n",
    "            \n",
    "        return error_msg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe to process        \n",
    "monday_df = pd.read_csv('D:\\\\data\\\\4025_METIS_embeddings\\\\source_data\\\\gdelt_20230616_bn_tl_id_20.csv')\n",
    "\n",
    "# Insert \\n after every sentence (may or may not be redundant at this point in time)\n",
    "def insert_newline(text):\n",
    "    sentences = text.split('. ')\n",
    "    mod_text = '\\n'.join(sentences)\n",
    "    return mod_text\n",
    "\n",
    "monday_df.loc[:,'norm_body'] = monday_df['norm_body'].apply(lambda x: insert_newline(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdddf4",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Auto-Detection of Languages to Downlaod</center>\n",
    "###### <center>Based on the lanugage ID's listed in the dataset.</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull unique items from language column\n",
    "lang_codes = monday_df['meta_body_language'].unique().tolist()\n",
    "\n",
    "# Replace NaN values with None\n",
    "lang_codes = [None if pd.isna(item) else item for item in lang_codes]\n",
    "\n",
    "# Remove None values from the list\n",
    "lang_codes = [item for item in lang_codes if item is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddf71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output\n",
    "lang_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc82ea3",
   "metadata": {},
   "source": [
    "---\n",
    "### <center> Model Loading and Pipeline Generation</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download each of the models\n",
    "\n",
    "for lang in lang_codes:\n",
    "    try:\n",
    "        stanza.download(lang)\n",
    "    except:\n",
    "        print('Language not found in Stanza holdings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pipelines for all non-english articles \n",
    "\n",
    "stanza_pipelines={}\n",
    "for lang in lang_codes:\n",
    "    try:\n",
    "        stanza_pipelines[lang] = stanza.Pipeline(lang,use_gpu=True)\n",
    "    except:\n",
    "        print(f'Unable to generate pipeline for {lang}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if not already downloaded\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Set pipeline for english articles\n",
    "en_nlp_pipe = nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the multi-language model in SpaCy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "# Load the English model in SpaCy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_en.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05deaa2",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Data Cleaning</center>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3792000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define precleaning functions\n",
    "\n",
    "def remove_patterns_and_text(text):\n",
    "    # List of exact text to remove\n",
    "    texts_to_remove = [\n",
    "        '- home- news- ecns wire- business- travel- photo- video- voices',\n",
    "        '- homeopinionpd voicepoliticsforeign affairsbusinessworldwe are chinasocietyculturesci-techvideophotosportstravelmilitarylifeexclusivespecialslanguages- chinese- japanese- french- spanish- russian- arabic- korean- german- portuguese- kiswahili- italian- kazakh- thai- malay- greekarchive',\n",
    "        '- portada- china- economÃ­a- mundo- iberoamÃ©rica- opiniÃ³n- ciencia-tecnologÃ­a- deportes- cultura- sociedad- viaje- fotosidiomas- chino- inglÃ©s- francÃ©s- ruso- espaÃ±ol- japonÃ©s- coreano- Ã¡rabe- alemÃ¡n- portuguÃ©s- italiano- kazajo- suajili- tailandÃ©s- malayo- griego- mÃ¡sdeportesfoto',\n",
    "        'globalink |',\n",
    "        'illustration: '\n",
    "        '- HomeOpinionPD VoicePoliticsForeign AffairsBusinessWorldWe Are ChinaSocietyCultureSci-TechVideoPhotoSportsTravelMilitaryLifeExclusiveSpecialsLanguages\\n- Chinese\\n- Japanese\\n- French\\n- Spanish\\n- Russian\\n- Arabic\\n- Korean\\n- German\\n- Portuguese\\n- Kiswahili\\n- Italian\\n- Kazakh\\n- Thai\\n- Malay\\n- Greek\\nArchive',\n",
    "        '- Home \\\n",
    "        - News\\\n",
    "        - Ecns Wire\\\n",
    "        - Business\\\n",
    "        - Travel\\\n",
    "        - Photo\\\n",
    "        - Video\\\n",
    "        - Voices',\n",
    "    ]\n",
    "    \n",
    "    # List of regex patterns to remove\n",
    "    patterns = [\n",
    "        'Xinhua \\| Updated: \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}',\n",
    "        'Source: Xinhua\\nEditor: huaxia\\n\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}',\n",
    "        r'\\b\\w+,\\s\\w+\\s\\d+\\s\\(\\w+\\)\\s--', \n",
    "        r'.*china.org.cn\\s\\|\\s.*à jour le\\s\\d{2}-\\d{2}-\\d{4}', \n",
    "        r'.*\\|\\supdated:\\s\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}', \n",
    "        r'source:\\sxinhuaeditor:\\shuaxia\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}',\n",
    "        r'^\\w+,\\s\\w+\\s\\d+,\\s\\d+', \n",
    "        r'^feature:\\s',     \n",
    "        r\"^Feature:\\s\", \n",
    "        r\"^Editor's Note:\\s\",\n",
    "        r'Autor:\\s',\n",
    "        r'\\(ecns\\) --',\n",
    "        r'\\(foto: vcg\\)',\n",
    "        r'\\(Foto: VCG\\)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    for removal_text in texts_to_remove:\n",
    "        text = text.replace(removal_text, '')\n",
    "    return text.strip()\n",
    "    \n",
    "def remove_before_dashes(text):\n",
    "    text = text.replace('---', '--')  # Replace '---' with '--'\n",
    "    parts = text.split('--')\n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()  # Return the part after '---'\n",
    "    else:\n",
    "        return text.strip()  # If '---' is not in the text, return the text as is\n",
    "    \n",
    "def remove_before_datetime(text):\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', text)\n",
    "    if match:\n",
    "        return text[match.end():].strip()  # Return the part after the datetime\n",
    "    else:\n",
    "        return text.strip()  # If no datetime is found, return the text as is\n",
    "    \n",
    "def remove_initial_caps_words(text):\n",
    "    return re.sub(r'^[A-Z]{3,}[:|,]?', '', text).strip()\n",
    "\n",
    "def remove_by_author(text):\n",
    "    return re.sub(r'^(By|by)[^-]*-','',text).strip()\n",
    "\n",
    "def remove_figure_captions(text):\n",
    "    return re.sub(r'\\d+/\\d+\\(.*?\\)', '', text).strip()\n",
    "\n",
    "def remove_before_keywords(text):\n",
    "    return re.sub(r'.*(- MÃ¡sDeportesFotos|-másdeportesfotos)', '', text).strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning functions\n",
    "monday_df['norm_body'] = monday_df['norm_body'].apply(remove_patterns_and_text).apply(remove_before_dashes).apply(remove_before_datetime).apply(remove_initial_caps_words).apply(remove_by_author).apply(remove_figure_captions).apply(remove_before_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "monday_df['norm_body'] = monday_df['norm_body'].str.replace('.', '. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980910d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataframe to only necessary columns\n",
    "selected_columns = ['uid','meta_body_language','norm_body']\n",
    "stripped_monday_df = monday_df[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f42c8a",
   "metadata": {},
   "source": [
    "# <center>*******************************************************************************************</center>\n",
    "# <center>DATA PROCESSING</center>\n",
    "# <center>*******************************************************************************************</center>\n",
    "# Processing Option # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169d2db",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Sentence Tokenization, Sans Embeddings</center>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae5177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your csv file into a pandas DataFrame\n",
    "df = monday_df.copy()\n",
    "\n",
    "# Prepare tqdm progress bar\n",
    "max_iter = len(df)\n",
    "with tqdm(total=max_iter, ncols=80) as pbar:\n",
    "    \n",
    "    # Initialize an empty list to hold new rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate through the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # for English\n",
    "        if row['meta_body_language'] == 'en': \n",
    "            doc = nlp_en(row['norm_body'])\n",
    "        # for multi-lingual\n",
    "        else:  \n",
    "            doc = nlp(row['norm_body'])\n",
    "\n",
    "        # Tokenize the article into sentences\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        \n",
    "        # Iterate through the sentences\n",
    "        for sentence in sentences:\n",
    "            # Create a new row with the sentence and the uid, and append it to new_rows\n",
    "            new_rows.append({'uid': row['uid'], 'sentence': sentence})\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Create a new DataFrame from the list of new rows\n",
    "sentence_df = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a90951",
   "metadata": {},
   "source": [
    "# <center>*******************************************************************************************</center>\n",
    "# Processing Option # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25cb16",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Sentence Tokenization WITH Embeddings & Attempted Language Identification</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run embeddings\n",
    "\n",
    "error_dict = {}\n",
    "embed_dict = {}\n",
    "\n",
    "\n",
    "def send_sentences(sentences,embed_dict,lang_code):\n",
    "    # Instatiate dictionary to store sentence embeddings with each loop iteration\n",
    "    sent_embed_dict = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            lang_detect = detect(sentence)\n",
    "        except:\n",
    "#             print(f'LangDetect could not identify language for sentence: {sentence}')\n",
    "            lang_detect = lang_code\n",
    "\n",
    "        try:\n",
    "            lang_id, _ = langid.classify(sentence)\n",
    "        except:\n",
    "#             print(f'Langid could not identify langauge for sentence: {sentence}')\n",
    "            lang_id = lang_code\n",
    "\n",
    "        final_lang = Counter([lang_code,lang_detect,lang_id]).most_common(1)[0][0]\n",
    "        # This is just to make sure that it is a string. Not really necessary, but I was tired\n",
    "        # of stuff breaking, so I threw it in for good measure.\n",
    "        if type(sentence) == str:\n",
    "\n",
    "            # Declare the payload for the API call. Send one sentence at a time.\n",
    "            payload = {\n",
    "            'content':sentence,\n",
    "            'lang':final_lang\n",
    "            }\n",
    "\n",
    "            # API CALL\n",
    "            LASER_gen = LASER_api_call(payload)\n",
    "\n",
    "            # Error response logging.\n",
    "            if isinstance(LASER_gen, str) and LASER_gen.startswith('Error'):\n",
    "                error_dict[uid] = LASER_gen\n",
    "                continue\n",
    "            else:\n",
    "                # If not error response, store to sentence embedding dictionary.\n",
    "                sent_embed_dict[sentence] = LASER_gen['embedding']\n",
    "        embed_dict[uid] = sent_embed_dict\n",
    "\n",
    "    return embed_dict\n",
    "\n",
    "# Logging of time to track process. It can be quite lengthy.\n",
    "start_time = time.time()\n",
    "\n",
    "start_time_str = time.strftime('%H:%M:%S', time.localtime(start_time))\n",
    "start_date_str = time.strftime('%Y-%m-%d', time.localtime(start_time))\n",
    "\n",
    "print(f'STARTED ON {start_date_str} @ {start_time_str}')\n",
    "\n",
    "#### ADDITIONAL DATA PRE-PROCESSING ####\n",
    "\n",
    "# removes non-printable unicode characters\n",
    "stripped_monday_df.loc[:,'norm_body'] = stripped_monday_df['norm_body'].apply(lambda text: ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C'))\n",
    "\n",
    "# remove html artifacts\n",
    "clean_re = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "stripped_monday_df.loc[:,'norm_body'] = stripped_monday_df['norm_body'].apply(lambda text: re.sub(clean_re, '', text))\n",
    "\n",
    "# all text should be lowercase before being tokenized\n",
    "stripped_monday_df.loc[:,'norm_body'] = stripped_monday_df['norm_body'].apply(lambda text: text.lower())\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "# Set max iteration for progress bar. Set variable to pickle load var name.\n",
    "max_iter = len(stripped_monday_df)\n",
    "\n",
    "with tqdm(total=max_iter, ncols=80) as pbar:\n",
    "    \n",
    "    # Iterate through the dataframe\n",
    "    for index, row in stripped_monday_df.iterrows():\n",
    "            # Set variables for each cell of the current row iteration.\n",
    "            text = row['norm_body']\n",
    "            lang_code = row['meta_body_language']\n",
    "            uid = row['uid']\n",
    "\n",
    "            # Set delimiter based on language code.\n",
    "\n",
    "            if lang_code in lang_codes:\n",
    "                if lang_code == 'en':\n",
    "                    sentences = en_nlp_pipe.sent_tokenize(text)\n",
    "                    embed_dict = send_sentences(sentences,embed_dict,lang_code)\n",
    "\n",
    "                else:\n",
    "                    if lang_code in stanza_pipelines.keys():\n",
    "                        Stanza_nlp_pipe = stanza_pipelines[lang_code]\n",
    "                        doc = Stanza_nlp_pipe(text)\n",
    "                        sentences = [sentence.text for sentence in doc.sentences]\n",
    "                        embed_dict = send_sentences(sentences,embed_dict,lang_code)#.embed_dict\n",
    "            else:\n",
    "                try:\n",
    "                    sentences = en_nlp_pipe.sent_tokenize(text)\n",
    "                    embed_dict = send_sentences(sentences,embed_dict,lang_code)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "# Calculate total amount of time it took to process the dataframe.\n",
    "total_time_seconds = time.time() - start_time\n",
    "total_time_minutes, total_time_seconds = divmod(total_time_seconds,60)\n",
    "total_time_hours, total_time_minutes = divmod(total_time_minutes,60)\n",
    "\n",
    "current_time_str = time.strftime('%H:%M:%S', time.localtime())\n",
    "current_date_str = time.strftime('%Y-%m-%d', time.localtime())\n",
    "\n",
    "# Display for reference.\n",
    "print(f'FINISHED ON {current_date_str} @ {current_time_str}\\n')\n",
    "print(f'*** TOTAL PROCESSING TIME: {int(total_time_hours):02d}:{int(total_time_minutes):02d}:{total_time_seconds:.2f} ***')\n",
    "\n",
    "\n",
    "##### PLay audio alert when done processing (if tab is actively selected) #####\n",
    "\n",
    "framerate = 44100\n",
    "play_time_seconds = 1\n",
    "\n",
    "# Change these to be higher for a higher pitch\n",
    "frequency1 = 880  # was 220\n",
    "frequency2 = 884  # was 224\n",
    "\n",
    "t = np.linspace(0, play_time_seconds, framerate*play_time_seconds)\n",
    "audio_data = np.sin(2*np.pi*frequency1*t) + np.sin(2*np.pi*frequency2*t)\n",
    "Audio(audio_data, rate=framerate, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfcca9",
   "metadata": {},
   "source": [
    "# <center>*******************************************************************************************</center>\n",
    "# <center>END OF DATA PROCESSING</center>\n",
    "# <center>*******************************************************************************************</center>\n",
    "---\n",
    "### <center>Check Unique Values from Processing</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows = new_df.drop_duplicates().shape[0]\n",
    "\n",
    "print(f\"Length of dataframe: {len(new_df['sentence'])}\")\n",
    "print(f'Number of unique translations: {unique_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6dec7b",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Export Data</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\exports\\\\LASER\\\\spacy_sent_tokenization.pickle','wb') as file:\n",
    "    pickle.dump(new_df,file,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export embedding dictionary as pickle. (Set to appropriate local directory.)\n",
    "with open('D:\\\\exports\\\\LASER\\\\cleaner_from_thurs_monday-6700_embed_dict_NLTK-Stanza-GPU_langDetect_spaces_added.pickle', 'wb') as file:\n",
    "    pickle.dump(embed_dict,file,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad51b6",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Reconstruct DataFrame for Data Validation</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988229c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct dictionary into user-friendly dataframe\n",
    "data=[]\n",
    "\n",
    "for uid, sentences in embed_dict.items():\n",
    "    \n",
    "    for sentence,embed in sentences.items():\n",
    "        \n",
    "        data.append({'uid':uid,'sentence':sentence,'embed':embed})\n",
    "        \n",
    "reconstructed_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run embeddings\n",
    "\n",
    "error_dict = {}\n",
    "embed_dict = {}\n",
    "\n",
    "\n",
    "def send_sentences(sentences,embed_dict,lang_code):\n",
    "    # Instatiate dictionary to store sentence embeddings with each loop iteration\n",
    "    sent_embed_dict = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            lang_detect = detect(sentence)\n",
    "        except:\n",
    "#             print(f'LangDetect could not identify language for sentence: {sentence}')\n",
    "            lang_detect = lang_code\n",
    "\n",
    "        try:\n",
    "            lang_id, _ = langid.classify(sentence)\n",
    "        except:\n",
    "#             print(f'Langid could not identify langauge for sentence: {sentence}')\n",
    "            lang_id = lang_code\n",
    "\n",
    "        final_lang = Counter([lang_code,lang_detect,lang_id]).most_common(1)[0][0]\n",
    "        # This is just to make sure that it is a string. Not really necessary, but I was tired\n",
    "        # of stuff breaking, so I threw it in for good measure.\n",
    "        if type(sentence) == str:\n",
    "\n",
    "            # Declare the payload for the API call. Send one sentence at a time.\n",
    "            payload = {\n",
    "            'content':sentence,\n",
    "            'lang':final_lang\n",
    "            }\n",
    "\n",
    "            # API CALL\n",
    "            LASER_gen = LASER_api_call(payload)\n",
    "\n",
    "            # Error response logging.\n",
    "            if isinstance(LASER_gen, str) and LASER_gen.startswith('Error'):\n",
    "                error_dict[uid] = LASER_gen\n",
    "                continue\n",
    "            else:\n",
    "                # If not error response, store to sentence embedding dictionary.\n",
    "                sent_embed_dict[sentence] = LASER_gen['embedding']\n",
    "        embed_dict[uid] = sent_embed_dict\n",
    "\n",
    "    return embed_dict\n",
    "\n",
    "# Logging of time to track process. It can be quite lengthy.\n",
    "start_time = time.time()\n",
    "\n",
    "start_time_str = time.strftime('%H:%M:%S', time.localtime(start_time))\n",
    "start_date_str = time.strftime('%Y-%m-%d', time.localtime(start_time))\n",
    "\n",
    "print(f'STARTED ON {start_date_str} @ {start_time_str}')\n",
    "\n",
    "#### DATA PRE-PROCESSING ####\n",
    "     # Courtesy of Ed #\n",
    "\n",
    "# removes non-printable unicode characters\n",
    "stripped_monday_df.loc[:,'norm_body'] = stripped_monday_df['norm_body'].apply(lambda text: ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C'))\n",
    "\n",
    "# remove html artifacts\n",
    "clean_re = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "stripped_monday_df.loc[:,'norm_body'] = stripped_monday_df['norm_body'].apply(lambda text: re.sub(clean_re, '', text))\n",
    "\n",
    "# all text should be lowercase before being tokenized\n",
    "stripped_monday_df.loc[:,'norm_body'] = stripped_monday_df['norm_body'].apply(lambda text: text.lower())\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "# Set max iteration for progress bar. Set variable to pickle load var name.\n",
    "max_iter = len(stripped_monday_df)\n",
    "\n",
    "with tqdm(total=max_iter, ncols=80) as pbar:\n",
    "    \n",
    "    # Iterate through the dataframe\n",
    "    for index, row in stripped_monday_df.iterrows():\n",
    "            # Set variables for each cell of the current row iteration.\n",
    "            text = row['norm_body']\n",
    "            lang_code = row['meta_body_language']\n",
    "            uid = row['uid']\n",
    "\n",
    "            # Set delimiter based on language code.\n",
    "\n",
    "            if lang_code in lang_codes:\n",
    "                if lang_code == 'en':\n",
    "                    sentences = en_nlp_pipe.sent_tokenize(text)\n",
    "                    embed_dict = send_sentences(sentences,embed_dict,lang_code)\n",
    "\n",
    "                else:\n",
    "                    if lang_code in stanza_pipelines.keys():\n",
    "                        Stanza_nlp_pipe = stanza_pipelines[lang_code]\n",
    "                        doc = Stanza_nlp_pipe(text)\n",
    "                        sentences = [sentence.text for sentence in doc.sentences]\n",
    "                        embed_dict = send_sentences(sentences,embed_dict,lang_code)#.embed_dict\n",
    "            else:\n",
    "                try:\n",
    "                    sentences = en_nlp_pipe.sent_tokenize(text)\n",
    "                    embed_dict = send_sentences(sentences,embed_dict,lang_code)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "# Calculate total amount of time it took to process the dataframe.\n",
    "total_time_seconds = time.time() - start_time\n",
    "total_time_minutes, total_time_seconds = divmod(total_time_seconds,60)\n",
    "total_time_hours, total_time_minutes = divmod(total_time_minutes,60)\n",
    "\n",
    "current_time_str = time.strftime('%H:%M:%S', time.localtime())\n",
    "current_date_str = time.strftime('%Y-%m-%d', time.localtime())\n",
    "\n",
    "# Display for reference.\n",
    "print(f'FINISHED ON {current_date_str} @ {current_time_str}\\n')\n",
    "print(f'*** TOTAL PROCESSING TIME: {int(total_time_hours):02d}:{int(total_time_minutes):02d}:{total_time_seconds:.2f} ***')\n",
    "\n",
    "\n",
    "##### PLay audio alert when done processing (if tab is actively selected) #####\n",
    "\n",
    "# framerate = 44100\n",
    "# play_time_seconds = 1\n",
    "\n",
    "# # Change these to be higher for a higher pitch\n",
    "# frequency1 = 880  # was 220\n",
    "# frequency2 = 884  # was 224\n",
    "\n",
    "# t = np.linspace(0, play_time_seconds, framerate*play_time_seconds)\n",
    "# audio_data = np.sin(2*np.pi*frequency1*t) + np.sin(2*np.pi*frequency2*t)\n",
    "# Audio(audio_data, rate=framerate, autoplay=True)\n",
    "audio_alert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa37b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv\n",
    "reconstructed_df.to_csv('D:\\\\exports\\\\LASER\\\\cleaner_from_thurs_sentence_level_langDetect_and_indiv_tokenization_models.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
